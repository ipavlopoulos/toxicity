{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aueb-nlp:bert@kaggle.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipavlopoulos/toxicity/blob/master/aueb_nlp_bert_kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK_y3cKhwo0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "37669e15-afb8-4a3d-d0d0-50616e8d7f81"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0514 15:34:55.776669 139670882322304 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_kdiPeCwz0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "\n",
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_PATH = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "# We'll set sequences to be at most 128 tokens long.\n",
        "max_seq_length = 128\n",
        "#max_seq_length = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3GEsn6Tw6yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24a52f3b-66d8-4c53-c9e7-632fb1717bab"
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq2Pu1l8w9St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-hw-nz_xFdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M686XYKbxHwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4uGw3Uz9tED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZfYczy2xQqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "!unzip cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tQyl9BE-Qse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np \n",
        "\n",
        "NUM_ROWS=10000\n",
        "#df = pd.read_csv(tf.gfile.Open(path, 'r'), nrows = NUM_ROWS)\n",
        "df = pd.read_csv(\"train.csv.zip\", nrows = NUM_ROWS)\n",
        "df = df[['id', 'target', 'comment_text']].sample(frac = 1)\n",
        "df['bool_target'] = (df['target'] > 0.5).apply(int)\n",
        "\n",
        "train, dev = train_test_split(df, test_size=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzzHHzKNxTaG",
        "colab_type": "code",
        "outputId": "ea43b118-7a31-4b1d-c697-410d96a36c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# short exploratory analysis\n",
        "train.head()\n",
        "train.bool_target.hist()\n",
        "train.target.hist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efff00675f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRhkAktQoA0CSGmUASFKjDABJatT/ApAAAEfSIYrpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnD0Wg-tApq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'comment_text'\n",
        "LABEL_COLUMN = 'bool_target'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvRj7X_uAyyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT0V2jG3BhSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "dev_InputExamples = dev.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbgoAMGSCApr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer_from_hub_module(model_path):\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(model_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module(BERT_MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93grkIR9CBJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, max_seq_length, tokenizer)\n",
        "dev_features = bert.run_classifier.convert_examples_to_features(dev_InputExamples, label_list, max_seq_length, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvfM9Ksv4rRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERT(tf.layers.Layer):\n",
        "\n",
        "    def __init__(self, n_fine_tune_layers=-1, output_representation='pooled_output', **kwargs):\n",
        "        \"\"\"\n",
        "        :param output_representation: 'pooled_output' for CLS toke or 'sequence_output' for one-to-one outputs\n",
        "        \"\"\"\n",
        "        assert output_representation in {\"pooled_output\", \"sequence_output\"}\n",
        "        self.bert = None\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_representation = output_representation\n",
        "        super(BERT, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(BERT_MODEL_PATH, trainable=self.trainable, \n",
        "                               name=\"{}_module\".format(self.name))\n",
        "        # Remove unused layers and set trainable parameters\n",
        "        trainable_vars = [var for var in self.bert.variables \n",
        "                             if not \"/cls/\" in var.name \n",
        "                             and not \"/pooler/\" in var.name\n",
        "                          ]\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        if self.n_fine_tune_layers > 0:\n",
        "          trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "          self._trainable_weights.append(var)\n",
        "          \n",
        "        # Update non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "          if var not in self._trainable_weights:\n",
        "            self._non_trainable_weights.append(var)\n",
        "          \n",
        "        super(BERT, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, input_mask, segment_ids = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
        "        out = self.bert(inputs=inputs, as_dict=True, signature='tokens')\n",
        "        #return out[self.output_representation]\n",
        "        if output_representation == \"pooled_output\":\n",
        "          return out['sequence_output'][:, 0, :] # return the CLS embedding\n",
        "        return out['sequence_output']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__aLi8Xtds05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "#output_representation=\"sequence_output\"\n",
        "output_representation=\"pooled_output\"\n",
        "\n",
        "def build_model(max_seq_length, output_representation=output_representation): \n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    bert_output = BERT(output_representation=output_representation, n_fine_tune_layers=3)(bert_inputs)\n",
        "    if output_representation == \"pooled_output\":\n",
        "      dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "    elif output_representation == \"sequence_output\":\n",
        "      dense = tf.keras.layers.GRU(128)(bert_output)\n",
        "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3EESlT-EUIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(features):\n",
        "  input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "  for f in features:\n",
        "    input_ids.append(f.input_ids)\n",
        "    input_masks.append(f.input_mask)\n",
        "    segment_ids.append(f.segment_ids)\n",
        "    labels.append(f.label_id)\n",
        "  return (\n",
        "    np.array(input_ids),\n",
        "    np.array(input_masks),\n",
        "    np.array(segment_ids),\n",
        "    np.array(labels).reshape(-1, 1),\n",
        "  )\n",
        "\n",
        "train_input_ids, train_input_masks, train_segment_ids, train_labels = get_features(train_features)\n",
        "dev_input_ids, dev_input_masks, dev_segment_ids, dev_labels = get_features(dev_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sdVmOK_EhEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(max_seq_length, output_representation=output_representation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmmyeJkKEpgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n",
        "\n",
        "model.fit(\n",
        "    [train_input_ids, train_input_masks, train_segment_ids], \n",
        "    train_labels,\n",
        "    validation_data=([dev_input_ids, dev_input_masks, dev_segment_ids], dev_labels),\n",
        "    epochs=1,\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzCnihiLFFUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict and evaluate\n",
        "predictions = model.predict([dev_input_ids, \n",
        "                             dev_input_masks, \n",
        "                             dev_segment_ids]) # predictions before we clear and reload model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(dev_labels, predictions)\n",
        "print('ROC AUC: {:.4f}'.format(auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTBWICyGFWYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "# save the predictions\n",
        "\n",
        "model.save('BertModelRNN.h5')\n",
        "tf.gfile.Copy('BertModelRNN.h5', '/BertModelRNN.h5')\n",
        "\n",
        "predictions_path = 'bertmodel_rnn_predictions.csv'\n",
        "resultFile = tf.gfile.Open(predictions_path, 'w')\n",
        "wr = csv.writer(resultFile)\n",
        "wr.writerows(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}